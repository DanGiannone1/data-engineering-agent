"""
Claude Code comparison - do the same transform process manually.
"""
import asyncio
import json
import os
import sys
import time

from dotenv import load_dotenv
load_dotenv()

sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))

# Patch ADLS client to use AzureCliCredential
import clients.adls as adls_module
from azure.identity import AzureCliCredential
from azure.storage.filedatalake import DataLakeServiceClient

def get_adls_client_cli():
    account_name = os.environ["ADLS_ACCOUNT_NAME"]
    return DataLakeServiceClient(
        account_url=f"https://{account_name}.dfs.core.windows.net",
        credential=AzureCliCredential(process_timeout=60),
    )

adls_module.get_adls_client = get_adls_client_cli

from tools.adls import read_mapping_spreadsheet, sample_source_data, read_spark_output
from tools.databricks import submit_spark_job, wait_for_spark_job


def generate_pyspark_code(mapping: dict, sample: dict, input_path: str, output_path: str) -> str:
    """Generate PySpark code based on mapping and sample data."""

    # Extract mapping rules from the spreadsheet
    mapping_sheet = mapping.get("Sheet1", mapping.get(list(mapping.keys())[0], {}))
    rows = mapping_sheet.get("sample_rows", [])

    # Build column mappings
    col_mappings = []
    for row in rows:
        source = row.get("source_column") or row.get("Source") or row.get("source")
        target = row.get("target_column") or row.get("Target") or row.get("target")
        rule = row.get("transformation") or row.get("Transformation") or row.get("rule") or "passthrough"
        if source and target:
            col_mappings.append({"source": source, "target": target, "rule": rule})

    # Generate PySpark code
    code = f'''# PySpark transformation generated by Claude Code
from pyspark.sql import functions as F
from pyspark.sql.types import *

input_path = "{input_path}"
output_path = "{output_path}"

# Read source data
df = spark.read.option("header", True).option("inferSchema", True).csv(input_path)

# Apply transformations
'''

    for cm in col_mappings:
        src, tgt, rule = cm["source"], cm["target"], cm["rule"].lower()
        if "upper" in rule:
            code += f'df = df.withColumn("{tgt}", F.upper(F.col("{src}")))\n'
        elif "date" in rule or "yyyy" in rule.lower():
            code += f'df = df.withColumn("{tgt}", F.date_format(F.to_date(F.col("{src}")), "yyyy-MM-dd"))\n'
        elif "round" in rule or "decimal" in rule:
            code += f'df = df.withColumn("{tgt}", F.round(F.col("{src}"), 2))\n'
        elif "map" in rule and "=" in rule:
            # Parse mapping like "completed=1, pending=2, failed=3"
            code += f'# Status mapping from rule: {rule}\n'
            code += f'df = df.withColumn("{tgt}", F.when(F.lower(F.col("{src}")) == "completed", 1).when(F.lower(F.col("{src}")) == "pending", 2).when(F.lower(F.col("{src}")) == "failed", 3).otherwise(None))\n'
        else:
            code += f'df = df.withColumn("{tgt}", F.col("{src}"))\n'

    # Select target columns
    target_cols = [cm["target"] for cm in col_mappings]
    code += f'''
# Select target columns
output_df = df.select({target_cols})

# Write output
output_df.write.mode("overwrite").parquet(output_path)
print(f"Wrote {{output_df.count()}} rows to {{output_path}}")
'''
    return code


def main():
    print("=" * 60)
    print("Claude Code - Manual Transform Process")
    print("=" * 60)

    client_id = "TEST_CLIENT"
    mapping_path = "TEST_CLIENT/mapping.xlsx"
    data_path = "TEST_CLIENT/transactions.csv"
    output_path = "TEST_CLIENT/output_claude"
    storage_account = os.environ["ADLS_ACCOUNT_NAME"]

    # Step 1: Read mapping
    print("\n[Step 1] Reading mapping...")
    start = time.time()
    mapping = read_mapping_spreadsheet(mapping_path)
    print(f"  Done in {time.time()-start:.1f}s")
    print(f"  Sheets: {list(mapping.keys())}")
    for sheet, data in mapping.items():
        print(f"  {sheet}: {data['row_count']} rows, columns: {data['columns']}")

    # Step 2: Sample source data
    print("\n[Step 2] Sampling source data...")
    start = time.time()
    sample = sample_source_data(data_path, 10)
    print(f"  Done in {time.time()-start:.1f}s")
    print(f"  Columns: {sample['columns']}")
    print(f"  Types: {sample['dtypes']}")
    print(f"  Sample row: {sample['sample_rows'][0]}")

    # Step 3: Generate PySpark code
    print("\n[Step 3] Generating PySpark code...")
    input_abfss = f"abfss://data@{storage_account}.dfs.core.windows.net/{data_path}"
    output_abfss = f"abfss://output@{storage_account}.dfs.core.windows.net/{output_path}"

    code = generate_pyspark_code(mapping, sample, input_abfss, output_abfss)
    print("  Generated code:")
    for line in code.split("\n")[:15]:
        print(f"    {line}")
    print("    ...")

    # Step 4: Submit Spark job
    print("\n[Step 4] Submitting Spark job...")
    start = time.time()
    run_id = submit_spark_job(code, client_id)
    print(f"  Submitted: run_id={run_id}")

    # Step 5: Wait for completion
    print("\n[Step 5] Waiting for completion...")
    result = wait_for_spark_job(run_id, poll_interval=15, timeout=600)
    print(f"  Done in {time.time()-start:.1f}s")
    print(f"  Result: {result}")

    if result.get("success"):
        # Step 6: Verify output
        print("\n[Step 6] Verifying output...")
        start = time.time()
        output = read_spark_output(output_path, 10)
        print(f"  Done in {time.time()-start:.1f}s")
        print(f"  Columns: {output['columns']}")
        print(f"  Row count: {output['row_count']}")
        print(f"  Sample: {output['sample_rows'][0]}")
    else:
        print(f"\n[ERROR] Job failed: {result.get('error_log')}")

    print("\n" + "=" * 60)
    print("Done")
    print("=" * 60)


if __name__ == "__main__":
    main()
